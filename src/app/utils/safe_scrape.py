import asyncio
import logging

from app.modules.tiktok_scraper.scrapers.channel import scrape_channel

log = logging.getLogger(__name__)

async def safe_scrape(url: str, retries: int = 3, delay: tuple = (1, 3)):
    """
    G·ªçi scraper c√≥ retry v√† x·ª≠ l√Ω l·ªói timeout, None, empty data.
    """
    for attempt in range(1, retries + 1):
        try:
            log.info(f"[Scrape Attempt {attempt}] ƒêang crawl: {url}")
            data = await scrape_channel(url)

            if not data:
                log.warning(f"[Scrape Attempt {attempt}] ‚ö†Ô∏è D·ªØ li·ªáu r·ªóng ho·∫∑c None")
                await asyncio.sleep(1)
                continue

            log.info(f"[Scrape Attempt {attempt}] ‚úÖ Th√†nh c√¥ng, crawl ƒë∆∞·ª£c {len(data)} b√†i vi·∫øt")
            return data

        except Exception as e:
            # Catch timeout ho·∫∑c l·ªói b·∫•t k·ª≥
            message = str(e)
            log.warning(f"[Scrape Attempt {attempt}] ‚ùå L·ªói: {message}")

            if "ERR::SCRAPE::OPERATION_TIMEOUT" in message:
                log.warning(f"‚è≥ Timeout t·ª´ Scrapfly - th·ª≠ l·∫°i sau {delay[1]}s")
            await asyncio.sleep(delay[0] + (attempt - 1) * (delay[1] - delay[0]))

    log.error(f"üî• Th·∫•t b·∫°i sau {retries} l·∫ßn retry: {url}")
    return None

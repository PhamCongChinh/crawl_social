import asyncio
import traceback
from app.modules.tiktok_scraper.scrapers.channel import scrape_channel
from app.modules.tiktok_scraper.services.channel import ChannelService
from app.modules.tiktok_scraper.services.source import SourceService
from app.utils.concurrency import limited_gather
from app.utils.delay import async_delay
from app.worker import celery_app


import logging

log = logging.getLogger(__name__)

from app.modules.tiktok_scraper.models.source import SourceModel
from app.modules.tiktok_scraper.services.channel import ChannelService
from app.config import mongo_connection

@celery_app.task(
    name="app.tasks.tiktok.channel.crawl_tiktok_channels",
    bind=True
)
def crawl_tiktok_channels(self, job_id: str, channel_id: str):
    print(f"Task {job_id} - {channel_id}")
    async def do_crawl():
        try:
            await mongo_connection.connect()
            sources = await SourceService.get_sources()
            log.info(f"üì¶ T·ªïng s·ªë source: {len(sources)}")
            # Trong h√†m async
            coroutines = []
            for idx, source in enumerate(sources):
                log.info(f"üïê [{idx+1}/{len(sources)}] {source.source_url}")
                data = source.model_dump(by_alias=True)
                data["_id"] = str(data["_id"])
                coroutines.append(crawl_tiktok_channel_direct(data))
            # Gi·ªõi h·∫°n 3 request Scrapfly ch·∫°y c√πng l√∫c
            await limited_gather(coroutines, limit=1)
            log.info(f"‚úÖ Task cha {job_id} ho√†n t·∫•t to√†n b·ªô")
        except Exception as e:
            log.error(e)
    return asyncio.run(do_crawl())

# Task con: x·ª≠ l√Ω crawl 1 source ‚Üí Scrapfly ‚Üí DB
async def crawl_tiktok_channel_direct(source: dict):
    try:
        await mongo_connection.connect()
        source_model = SourceModel(**source)
        log.info(f"üîç Crawling source: {source_model.source_url}")
        data = await safe_scrape_with_delay(source_model.source_url)
        if not data:
            log.warning(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ {source_model.source_url}")
            return
        result = await ChannelService.upsert_channels_bulk(data, source=source_model)
        log.info(
            f"‚úÖ Upsert xong {source_model.source_url}: matched={result.matched_count}, "
            f"inserted={result.upserted_count}, modified={result.modified_count}"
        )

    except Exception as e:
        log.error(f"‚ùå L·ªói crawl {source.get('source_url')}: {e}")


async def safe_scrape_with_delay(url: str, max_retries: int = 3):
    for attempt in range(1, max_retries + 1):
        try:
            data = await scrape_channel(url)
            await async_delay(2, 4)  # ƒê·∫£m b·∫£o browser session tr∆∞·ªõc shutdown
            return data
        except Exception as e:
            log.warning(f"‚ùó Attempt {attempt}/{max_retries} - L·ªói scrape: {e}")
            if attempt < max_retries:
                await async_delay(5, 8)  # Delay l√¢u h∆°n n·∫øu l·ªói
            else:
                log.error(f"‚ùå B·ªè qua URL sau {max_retries} l·∫ßn th·ª≠: {url}")
                return None
import asyncio
from typing import List
from app.modules.tiktok_scraper.scrapers.channel import scrape_channel
from app.modules.tiktok_scraper.services.channel import ChannelService
from app.modules.tiktok_scraper.services.source import SourceService
from app.utils.concurrency import limited_gather
from app.utils.delay import async_delay
from app.worker import celery_app
import logging
log = logging.getLogger(__name__)
from app.modules.tiktok_scraper.models.source import SourceModel
from app.modules.tiktok_scraper.services.channel import ChannelService
from app.config import mongo_connection

@celery_app.task(
    name="app.tasks.tiktok.channel.crawl_tiktok_channels",
    bind=True
)
def crawl_tiktok_channels(self, job_id: str, channel_id: str):
    log.info(f"Task {job_id} - {channel_id} b·∫Øt ƒë·∫ßu")
    async def do_crawl():
        try:
            await mongo_connection.connect()
            sources = await SourceService.get_sources()
            log.info(f"üì¶ T·ªïng s·ªë source: {len(sources)}")
            batch_size = 5  # S·ªë l∆∞·ª£ng source m·ªói l·∫ßn crawl
            batches = _chunk_sources(sources, batch_size)
            log.info(f"üì¶ Chia th√†nh {len(batches)} batch, m·ªói batch {batch_size} ngu·ªìn")
            for batch_index, batch in enumerate(batches, start=1):
                log.info(f"üöÄ ƒêang x·ª≠ l√Ω batch {batch_index}/{len(batches)} v·ªõi {len(batch)} ngu·ªìn")
                coroutines = []
                for idx, source in enumerate(batch):
                    overall_idx = (batch_index - 1) * batch_size + idx + 1
                    log.info(f"üïê [{overall_idx}/{len(sources)}] {source.source_url}")
                    data = source.model_dump(by_alias=True)
                    data["_id"] = str(data["_id"])
                    coroutines.append(crawl_tiktok_channel_direct(data))
                await limited_gather(coroutines, limit=3)  # Gi·ªõi h·∫°n 3 request Scrapfly ch·∫°y c√πng l√∫c
                await asyncio.sleep(2)  # ngh·ªâ 2 gi√¢y gi·ªØa batch

            # Trong h√†m async
            # coroutines = []
            # for idx, source in enumerate(sources):
            #     log.info(f"üïê [{idx+1}/{len(sources)}] {source.source_url}")
            #     data = source.model_dump(by_alias=True)
            #     data["_id"] = str(data["_id"])
            #     coroutines.append(crawl_tiktok_channel_direct(data))
            # # Gi·ªõi h·∫°n 3 request Scrapfly ch·∫°y c√πng l√∫c
            # await limited_gather(coroutines, limit=2)

            log.info(f"‚úÖ Task cha {job_id} ho√†n t·∫•t to√†n b·ªô")
        except Exception as e:
            log.error(e)
    return asyncio.run(do_crawl())

def _chunk_sources(sources: List, batch_size: int) -> List[List]:
    return [sources[i:i + batch_size] for i in range(0, len(sources), batch_size)]

# Task con: x·ª≠ l√Ω crawl 1 source ‚Üí Scrapfly ‚Üí DB
async def crawl_tiktok_channel_direct(source: dict):
    try:
        await mongo_connection.connect()
        source_model = SourceModel(**source)
        log.info(f"üîç Crawling source: {source_model.source_url}")
        data = await safe_scrape_with_delay(source_model.source_url)
        if not data:
            log.warning(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ {source_model.source_url}")
            return
        result = await ChannelService.upsert_channels_bulk(data, source=source_model)
        log.info(
            f"‚úÖ Upsert xong {source_model.source_url}: matched={result.matched_count}, "
            f"inserted={result.upserted_count}, modified={result.modified_count}"
        )
    except Exception as e:
        log.error(f"‚ùå L·ªói crawl {source.get('source_url')}: {e}")


async def safe_scrape_with_delay(url: str, max_retries: int = 3):
    for attempt in range(1, max_retries + 1):
        try:
            data = await scrape_channel(url)
            await async_delay(2, 4)  # ƒê·∫£m b·∫£o browser session tr∆∞·ªõc shutdown
            return data
        except Exception as e:
            log.warning(f"‚ùó Attempt {attempt}/{max_retries} - L·ªói scrape: {e}")
            if attempt < max_retries:
                await async_delay(5, 8)  # Delay l√¢u h∆°n n·∫øu l·ªói
            else:
                log.error(f"‚ùå B·ªè qua URL sau {max_retries} l·∫ßn th·ª≠: {url}")
                return None
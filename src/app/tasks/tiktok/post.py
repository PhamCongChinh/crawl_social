import asyncio
from datetime import datetime
import json
from typing import List
from zoneinfo import ZoneInfo

from bson import Int64
from app.modules.elastic_search.service import postToES, postToESUnclassified
from app.modules.tiktok_scraper.models.channel import ChannelModel
from app.modules.tiktok_scraper.scrapers.comment import scrape_comments
from app.modules.tiktok_scraper.scrapers.post import scrape_posts
from app.modules.tiktok_scraper.services.channel import ChannelService
from app.modules.tiktok_scraper.services.post import PostService
from app.utils.concurrency import limited_gather
from app.utils.delay import async_delay
from app.worker import celery_app
from beanie.operators import In, And

import logging
log = logging.getLogger(__name__)

from app.config import mongo_connection
VN_TZ = ZoneInfo("Asia/Ho_Chi_Minh")


@celery_app.task(
    queue="tiktok_platform",
    name="app.tasks.tiktok.channel.crawl_tiktok_posts_hourly"
)
def crawl_tiktok_posts_hourly(job_name:str, crawl_type: str):
    async def do_crawl():
        try:
            log.info("L·∫•y d·ªØ li·ªáu b√†i vi·∫øt h·∫±ng ng√†y")
            await mongo_connection.connect()
            videos = await ChannelService.get_channels_posts_hourly()
            if len(videos) > 0:
                log.info("Kh√¥ng c√≥ d·ªØ li·ªáu trong ng√†y")
                await mongo_connection.disconnect()
                return
            ids = [str(v.id) for v in videos]
            video_dicts = [v.model_dump() for v in videos]
            await ChannelModel.find(In(ChannelModel.id, ids)).update_many({"$set": {"status": "processing"}})
            log.info(f"üöÄ ƒêang c√†o {len(video_dicts)} video")
            data_list_classified = []
            data_list_unclassified = []
            for index, video in enumerate(video_dicts):
                log.info(f"üïê [{index+1}/{len(video_dicts)}] {video['id']}")
                if video["org_id"] == 0:
                    data_list_unclassified.append(video)
                else:
                    data_list_classified.append(video)
            # Crawl & post classified
            if data_list_classified:
                post_data_classified = await crawl_tiktok_post_list_direct_classified(data_list_classified)
                if post_data_classified:
                    await postToES(post_data_classified)
                    log.info(f"ƒê√£ th√™m {len(post_data_classified)} video ƒë√£ ph√¢n lo·∫°i v√†o ElasticSearch")
            # Crawl & post unclassified
            if data_list_unclassified:
                post_data_unclassified = await crawl_tiktok_post_list_direct_unclassified(data_list_unclassified)
                log.info(f"post_data: {post_data_unclassified}")
                if post_data_unclassified:
                    await postToESUnclassified(post_data_unclassified)
                    log.info(f"ƒê√£ th√™m {len(post_data_unclassified)} video ch∆∞a ph√¢n lo·∫°i v√†o ElasticSearch")
            log.info(f"üì¶ T·ªïng s·ªë video ƒë√£ l·∫•y: {len(data_list_classified) + len(data_list_unclassified)}")
            await mongo_connection.disconnect()
        except Exception as e:
            log.error(f"‚ùå L·ªói khi c√†o d·ªØ li·ªáu: {e}")
            await mongo_connection.disconnect()
    return asyncio.run(do_crawl())


async def crawl_tiktok_post_list_direct_classified(channels: list[dict]):
    try:
        log.info(f"üì¶ T·ªïng s·ªë channel: {len(channels)}")
        urls = [item["id"] for item in channels]
        posts_data = []
        data = await scrape_posts(urls)
        processed_ids = []
        for channel in channels:
            for post in data:
                if post.get("id") == channel["id"].strip("/").split("/")[-1]:
                    flatten = flatten_post_data(post, channel)
                    posts_data.append(flatten)
                    processed_ids.append(channel["id"])  # L∆∞u l·∫°i _id c·∫ßn ƒë√°nh d·∫•u
        if processed_ids:
            await ChannelModel.find(In(ChannelModel.id, processed_ids)).update_many(
                {"$set": {"status": "done"}}
            )
        await async_delay(2, 4)  # ƒê·∫£m b·∫£o browser session tr∆∞·ªõc shutdown
        return posts_data

    except Exception as e:
        log.error(e)

async def crawl_tiktok_post_list_direct_unclassified(channels: list[dict]):
    try:
        # await mongo_connection.connect()
        log.info(f"üì¶ T·ªïng s·ªë channel: {len(channels)}")
        urls = [item["id"] for item in channels]
        posts_data = []
        data = await scrape_posts(urls)
        processed_ids = []
        for channel in channels:
            for post in data:
                if post.get("id") == channel["id"].strip("/").split("/")[-1]:
                    flatten = flatten_post_data_unclassified(post, channel)
                    posts_data.append(flatten)
                    processed_ids.append(channel["id"])  # L∆∞u l·∫°i _id c·∫ßn ƒë√°nh d·∫•u
        if processed_ids:
            await ChannelModel.find(In(ChannelModel.id, processed_ids)).update_many(
                {"$set": {"status": "done"}}
            )

        await async_delay(2, 4)  # ƒê·∫£m b·∫£o browser session tr∆∞·ªõc shutdown
        return posts_data

    except Exception as e:
        log.error(e)

def flatten_post_data(raw: dict, channel: dict) -> dict:
    return {
        "id": raw.get("id", None),
        "doc_type": 1,  # POST = 1, COMMENT = 2
        "crawl_source": 2,
        "crawl_source_code": channel.get("source_channel", None),
        "pub_time": Int64(int(raw.get("createTime", 0))),
        "crawl_time": int(datetime.now(VN_TZ).timestamp()),
        "org_id": channel.get("org_id", None),
        "subject_id": raw.get("subject_id", None),
        "title": raw.get("title", None),
        "description": raw.get("description", None),
        "content": raw.get("desc", None),
        "url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}/video/{raw['id']}",
        "media_urls": "[]",
        "comments": raw.get("stats", {}).get("commentCount", 0),
        "shares": raw.get("stats", {}).get("shareCount", 0),
        "reactions": raw.get("stats", {}).get("diggCount", 0),
        "favors": int(raw.get("stats", {}).get("collectCount", 0) or 0),
        "views": raw.get("stats", {}).get("playCount", 0),
        "web_tags": "[]",#json.dumps(raw.get("diversificationLabels", [])),
        "web_keywords": "[]",# json.dumps(raw.get("suggestedWords", [])),
        "auth_id": raw.get("author", {}).get("id", ""),
        "auth_name": raw.get("author", {}).get("nickname", ""),
        "auth_type": 1,
        "auth_url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}",
        "source_id": raw.get("id", None),
        "source_type": channel.get("source_type", None),
        "source_name": channel.get("source_name", None),
        "source_url": channel.get("source_url", None),
        "reply_to": None,
        "level": None,
        "sentiment": 0,
        "isPriority": True,
        "crawl_bot": "tiktok_post",
    }

def flatten_post_data_unclassified (raw: dict, channel: dict) -> dict:
    return {
        # "id": raw.get("id", None),
        "doc_type": 1,  # POST = 1, COMMENT = 2
        "crawl_source": 2,
        "crawl_source_code": channel.get("source_channel", None),
        "pub_time": Int64(int(raw.get("createTime", 0))),
        "crawl_time": int(datetime.now(VN_TZ).timestamp()),
        # "org_id": channel.get("org_id", None),
        "subject_id": raw.get("subject_id", None),
        "title": raw.get("title", None),
        "description": raw.get("description", None),
        "content": raw.get("desc", None),
        "url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}/video/{raw['id']}",
        "media_urls": "[]",
        "comments": raw.get("stats", {}).get("commentCount", 0),
        "shares": raw.get("stats", {}).get("shareCount", 0),
        "reactions": raw.get("stats", {}).get("diggCount", 0),
        "favors": int(raw.get("stats", {}).get("collectCount", 0) or 0),
        "views": raw.get("stats", {}).get("playCount", 0),
        "web_tags": "[]",#json.dumps(raw.get("diversificationLabels", [])),
        "web_keywords": "[]",# json.dumps(raw.get("suggestedWords", [])),
        "auth_id": raw.get("author", {}).get("id", ""),
        "auth_name": raw.get("author", {}).get("nickname", ""),
        "auth_type": 1,
        "auth_url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}",
        "source_id": raw.get("id", None),
        "source_type": channel.get("source_type", None),
        "source_name": channel.get("source_name", None),
        "source_url": channel.get("source_url", None),
        "reply_to": None,
        "level": None,
        "sentiment": 0,
        # "isPriority": False,
        "crawl_bot": "tiktok_post",
    }


@celery_app.task(
    name="app.tasks.tiktok.post.crawl_tiktok_posts",
    bind=True
)
def crawl_tiktok_posts(self, job_id: str, channel_id: str):
    print(f"Task {job_id} - {channel_id}")
#     async def do_crawl():
#         try:
#             await mongo_connection.connect()
#             channels = await ChannelService.get_channels_crawl()
#             # channels = await ChannelService.get_videos_to_crawl()
#             log.info(f"üöÄ ƒêang c√†o {len(channels)}")

#             # # Trong h√†m async
#             # coroutines = []
#             # for idx, channel in enumerate(channels):
#             #     log.info(f"üïê [{idx+1}/{len(channels)}] {channel.id}")
#             #     data = channel.model_dump(by_alias=True)
#             #     data["_id"] = str(data["_id"])
#             #     coroutines.append(crawl_tiktok_post_direct(data))
#             # # Gi·ªõi h·∫°n 3 request Scrapfly ch·∫°y c√πng l√∫c
#             # await limited_gather(coroutines, limit=1)

#             # Tu·∫ßn t·ª±

#             # LIMIT = 50
#             batch_size = 3  # S·ªë l∆∞·ª£ng source m·ªói l·∫ßn crawl
#             batches = _chunk_sources(channels, batch_size)
#             log.info(f"üì¶ Chia th√†nh {len(batches)} batch, m·ªói batch {batch_size} ngu·ªìn")
#             for batch_index, batch in enumerate(batches, start=1):
#                 log.info(f"üöÄ ƒêang x·ª≠ l√Ω batch {batch_index}/{len(batches)} v·ªõi {len(batch)} ngu·ªìn")

#                 data_list = []
#                 data_list_unclassified = []
#                 for idx, channel in enumerate(channels):
#                     log.info(f"üïê [{idx+1}/{len(channels)}] {channel.id}")
#                     data = channel.model_dump(by_alias=True)
#                     data["_id"] = str(data["_id"])
#                     if data["org_id"] == 0:
#                         # N·∫øu org_id = 0 th√¨ post v√†o unclassified
#                         data_list_unclassified.append(data)
#                         log.info(f"Th√™m v√†o unclassified: {data['_id']}")
#                     else:
#                         # N·∫øu org_id != 0 th√¨ post v√†o classified
#                         data_list.append(data)
#                         log.info(f"Th√™m v√†o classified: {data['_id']}")
#                     # if idx + 1 >= LIMIT:
#                     #     log.info(f"üõë ƒê√£ x·ª≠ l√Ω {LIMIT} b√†i, d·ª´ng t·∫°m.")
#                     #     break

#                 if len(data_list) > 0:
#                     log.info(f"üì¶ T·ªïng s·ªë channel classified: {len(data_list)}")
#                     # G·ªçi h√†m x·ª≠ l√Ω 1 l·∫ßn
#                     post_data = await crawl_tiktok_post_list_direct(data_list) # M·ª•c ƒë√≠ch l√† c√≥ d·ªØ li·ªáu ƒë·ªÉ post l√™n ES
#                     if len(post_data) > 0:
#                         result = await postToES(post_data)
#                         log.info(f"‚úÖ ƒê√£ post {len(post_data)} b√†i vi·∫øt classified l√™n ES")

#                 if len(data_list_unclassified) > 0:
#                     log.info(f"üì¶ T·ªïng s·ªë channel unclassified: {len(data_list_unclassified)}")
#                     post_data_unclassified = await crawl_tiktok_post_list_direct_unclassified(data_list_unclassified)
#                     print(f"post_data_unclassified: {post_data_unclassified}")
#                     if len(post_data_unclassified) > 0:
#                         result_unclassified = await postToESUnclassified(post_data_unclassified)
#                     log.info(f"‚úÖ ƒê√£ post {len(post_data_unclassified)} b√†i vi·∫øt unclassified l√™n ES")
            

#             return None
#         except Exception as e:
#             log.error(e)
#     return asyncio.run(do_crawl())




# def _chunk_sources(sources: List, batch_size: int) -> List[List]:
#     return [sources[i:i + batch_size] for i in range(0, len(sources), batch_size)]

# async def crawl_tiktok_post_list_direct(channels: list[dict]):
#     try:
#         await mongo_connection.connect()
#         log.info(f"üì¶ T·ªïng s·ªë channel: {len(channels)}")
#         urls = [item["_id"] for item in channels]
#         posts_data = []
#         data = await scrape_posts(urls)
#         for channel in channels:
#             for post in data:
#                 if post.get("id") == channel["_id"].strip("/").split("/")[-1]:
#                     flatten = flatten_post_data_1(post, channel)
#                     await ChannelService.channel_crawled(channel["_id"])
#                     posts_data.append(flatten)

#         await async_delay(2, 4)  # ƒê·∫£m b·∫£o browser session tr∆∞·ªõc shutdown
#         return posts_data

#     except Exception as e:
#         log.error(e)

# async def crawl_tiktok_post_list_direct_unclassified(channels: list[dict]):
#     try:
#         await mongo_connection.connect()
#         log.info(f"üì¶ T·ªïng s·ªë channel: {len(channels)}")
#         urls = [item["_id"] for item in channels]
#         posts_data = []
#         data = await scrape_posts(urls)
#         for channel in channels:
#             for post in data:
#                 if post.get("id") == channel["_id"].strip("/").split("/")[-1]:
#                     flatten = flatten_post_data_unclassified_1(post, channel)
#                     await ChannelService.channel_crawled(channel["_id"])
#                     posts_data.append(flatten)

#         await async_delay(2, 4)  # ƒê·∫£m b·∫£o browser session tr∆∞·ªõc shutdown
#         return posts_data

#     except Exception as e:
#         log.error(e)

# def flatten_post_data_1(raw: dict, channel: dict) -> dict:
#     return {
#         "id": raw.get("id", None),
#         "doc_type": 1,  # POST = 1, COMMENT = 2
#         "crawl_source": 2,
#         "crawl_source_code": channel.get("source_channel", None),
#         "pub_time": Int64(int(raw.get("createTime", 0))),
#         "crawl_time": int(datetime.now(VN_TZ).timestamp()),
#         "org_id": channel.get("org_id", None),
#         "subject_id": raw.get("subject_id", None),
#         "title": raw.get("title", None),
#         "description": raw.get("description", None),
#         "content": raw.get("desc", None),
#         "url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}/video/{raw['id']}",
#         "media_urls": "[]",
#         "comments": raw.get("stats", {}).get("commentCount", 0),
#         "shares": raw.get("stats", {}).get("shareCount", 0),
#         "reactions": raw.get("stats", {}).get("diggCount", 0),
#         "favors": int(raw.get("stats", {}).get("collectCount", 0) or 0),
#         "views": raw.get("stats", {}).get("playCount", 0),
#         "web_tags": json.dumps(raw.get("diversificationLabels", [])),
#         "web_keywords": json.dumps(raw.get("suggestedWords", [])),
#         "auth_id": raw.get("author", {}).get("id", ""),
#         "auth_name": raw.get("author", {}).get("nickname", ""),
#         "auth_type": 1,
#         "auth_url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}",
#         "source_id": raw.get("id", None),
#         "source_type": channel.get("source_type", None),
#         "source_name": channel.get("source_name", None),
#         "source_url": channel.get("source_url", None),
#         "reply_to": raw.get("replyTo", None),
#         "level": raw.get("level", None) or 0,
#         "sentiment": 0,
#         "isPriority": True,
#         "crawl_bot": "tiktok_post",
#     }

# def flatten_post_data_unclassified_1 (raw: dict, channel: dict) -> dict:
#     return {
#         # "id": raw.get("id", None),
#         "doc_type": 1,  # POST = 1, COMMENT = 2
#         "crawl_source": 2,
#         "crawl_source_code": channel.get("source_channel", None),
#         "pub_time": Int64(int(raw.get("createTime", 0))),
#         "crawl_time": int(datetime.now(VN_TZ).timestamp()),
#         # "org_id": channel.get("org_id", None),
#         "subject_id": raw.get("subject_id", None),
#         "title": raw.get("title", None),
#         "description": raw.get("description", None),
#         "content": raw.get("desc", None),
#         "url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}/video/{raw['id']}",
#         "media_urls": "[]",
#         "comments": raw.get("stats", {}).get("commentCount", 0),
#         "shares": raw.get("stats", {}).get("shareCount", 0),
#         "reactions": raw.get("stats", {}).get("diggCount", 0),
#         "favors": int(raw.get("stats", {}).get("collectCount", 0) or 0),
#         "views": raw.get("stats", {}).get("playCount", 0),
#         "web_tags": json.dumps(raw.get("diversificationLabels", [])),
#         "web_keywords": json.dumps(raw.get("suggestedWords", [])),
#         "auth_id": raw.get("author", {}).get("id", ""),
#         "auth_name": raw.get("author", {}).get("nickname", ""),
#         "auth_type": 1,
#         "auth_url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}",
#         "source_id": raw.get("id", None),
#         "source_type": channel.get("source_type", None),
#         "source_name": channel.get("source_name", None),
#         "source_url": channel.get("source_url", None),
#         "reply_to": raw.get("replyTo", None),
#         "level": raw.get("level", None) or 0,
#         "sentiment": 0,
#         # "isPriority": False,
#         "crawl_bot": "tiktok_post",
#     }
































# async def crawl_tiktok_post_direct(channel: dict):
#     try:
#         await mongo_connection.connect()
#         channel_model = ChannelModel(**channel)

#         log.info(f"üîç Crawling source: {channel_model.id}")
        
#         data = await safe_scrape_with_delay(channel_model.id)
        
#         if not data or not data[0]:
#             log.warning(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ {channel_model.id}")
#             return
        
#         await async_delay(2,4)

#         # comments = await safe_scrape_with_delay_comments(channel_model.id) # Tr·∫£ 1 list comment
#         # comments = await safe_scrape_with_delay_comments("https://www.tiktok.com/@vietjetvietnam/video/7520222989380553992")
#         # if not comments or not comments[0]:
#         #     log.warning(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu comments t·ª´ {channel_model.id}")
#         #     return
#         # comments_to_es = []
#         # for comment in comments:
#         #     c = flatten_post_data_comment(comment, channel=channel_model)
#         #     comments_to_es.append(c)
#         # await async_delay(1,2)
#         # print(comments_to_es)
#         # await postToES(comments_to_es)

#         # Ph√¢n lo·∫°i ·ªü ƒë√¢y
#         if channel_model.org_id == 0:
#             post = flatten_post_data_unclassified(data[0], channel=channel_model)
#             log.info(f"‚úÖ Th√™m v√†o flatten org_id = 0: {channel_model.id}")
#             print(f"post: {post}")
#             await postToESUnclassified([post])
            
#         else:
#             post = flatten_post_data(data[0], channel=channel_model)
#             log.info(f"‚úÖ Th√™m v√†o flatten org_id != 0: {post.get('id')}")
#             await postToES([post])

#         await async_delay(2,3)

#         return {"status": "success"}
#     except Exception as e:
#         # log.error(f"‚ùå L·ªói crawl {post.get('source_url')}: {e}")
#         log.error(e)


# async def safe_scrape_with_delay(url: str, max_retries: int = 3):
#     for attempt in range(1, max_retries + 1):
#         try:
#             data = await scrape_posts([url])
#             await async_delay(2, 4)  # ƒê·∫£m b·∫£o browser session tr∆∞·ªõc shutdown
#             return data
#         except Exception as e:
#             log.warning(f"‚ùó Attempt {attempt}/{max_retries} - L·ªói scrape: {e}")
#             if attempt < max_retries:
#                 await async_delay(5, 8)  # Delay l√¢u h∆°n n·∫øu l·ªói
#             else:
#                 log.error(f"‚ùå B·ªè qua URL sau {max_retries} l·∫ßn th·ª≠: {url}")
#                 return None
            


# def flatten_post_data(raw: dict, channel: ChannelModel) -> dict:
#     return {
#         "id": raw.get("id", None),
#         "doc_type": 1, # POST = 1, COMMENT = 2
#         "crawl_source": 2,
#         "crawl_source_code": channel.source_channel,
#         "pub_time": Int64(int(raw.get("createTime", 0))),
#         "crawl_time": int(datetime.now(VN_TZ).timestamp()),
#         "org_id": channel.org_id,
#         "subject_id": raw.get("subject_id", None),
#         "title": raw.get("title", None),
#         "description": raw.get("description", None),
#         "content": raw.get("desc", None),
#         "url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}/video/{raw['id']}",
#         "media_urls": "[]",
#         "comments": raw.get("stats", {}).get("commentCount", 0),
#         "shares": raw.get("stats", {}).get("shareCount", 0),
#         "reactions": raw.get("stats", {}).get("diggCount", 0),
#         "favors": int(raw.get("stats", {}).get("collectCount", 0) or 0),
#         "views": raw.get("stats", {}).get("playCount", 0),
#         "web_tags": json.dumps(raw.get("diversificationLabels", [])),
#         "web_keywords": json.dumps(raw.get("suggestedWords", [])),
#         "auth_id": raw.get("author", {}).get("id", ""),
#         "auth_name": raw.get("author", {}).get("nickname", ""),
#         "auth_type": 1,
#         "auth_url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}",
#         "source_id": raw.get("id", None),
#         "source_type": channel.source_type,
#         "source_name": channel.source_name,
#         "source_url": channel.source_url,
#         "reply_to": raw.get("replyTo", None),
#         "level": raw.get("level", None) or 0,
#         "sentiment": 0,
#         "isPriority": True,
#         "crawl_bot": "tiktok_post",
#     }


# def flatten_post_data_unclassified (raw: dict, channel: ChannelModel) -> dict:
#     return {
#         # "id": raw.get("id", ""),
#         "doc_type": 1, # POST = 1, COMMENT = 2
#         "crawl_source": 2,
#         "crawl_source_code": channel.source_channel,
#         "pub_time": Int64(int(raw.get("createTime", 0))),
#         "crawl_time": int(datetime.now(VN_TZ).timestamp()),
#         # "org_id": None,#channel.org_id,
#         "subject_id": raw.get("subject_id", None),
#         "title": raw.get("title", None),
#         "description": raw.get("description", None),
#         "content": raw.get("desc", None),
#         "url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}/video/{raw['id']}",
#         "media_urls": "[]",
#         "comments": raw.get("stats", {}).get("commentCount", 0),
#         "shares": raw.get("stats", {}).get("shareCount", 0),
#         "reactions": raw.get("stats", {}).get("diggCount", 0),
#         "favors": int(raw.get("stats", {}).get("collectCount", 0) or 0),
#         "views": raw.get("stats", {}).get("playCount", 0),
#         "web_tags": json.dumps(raw.get("diversificationLabels", [])),
#         "web_keywords": json.dumps(raw.get("suggestedWords", [])),
#         "auth_id": raw.get("author", {}).get("id", None),
#         "auth_name": raw.get("author", {}).get("nickname", None),
#         "auth_type": 1,
#         "auth_url": f"https://www.tiktok.com/@{raw.get('author', {}).get('uniqueId', '')}",
#         "source_id": raw.get("id", None),
#         "source_type": channel.source_type,
#         "source_name": channel.source_name,
#         "source_url": channel.source_url,
#         "reply_to": raw.get("replyTo", None),
#         "level": raw.get("level", None) or 0,
#         "sentiment": 0,
#         "isPriority": False,
#         "crawl_bot": "tiktok_post",
#     }